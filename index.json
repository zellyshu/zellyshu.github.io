[{"content":"Intro 최근 발생한 Log4j 보안 취약점 사태로 인하여 시끌시끌한 가운데, Tableau에서도 Log4j 보안 취약점에 대한 대응방안을 내놓았다. 따라서 이와 관련된 내용을 정리하고, 실제 업데이트를 해본 사항을 기록해둔다.\n Tableau에서의 Log4j Log4j Apache 라이브러리를 사용하는 Tableau 제품은 보안 취약점을 가지고 있으며, 이에 영향을 받는 제품들은 Log4j 보안 취약점 이슈가 해결된 패치 버전으로 업그레이드하거나, jndilookup.class를 삭제하여야 한다.\n https://kb.tableau.com/articles/issue/Apache-Log4j2-vulnerability-Log4shell\n  이슈 대응방안\n Log4j 보안 취약점 이슈가 해결된 패치 버전으로 업그레이드 jndilookup.class 삭제    Log4j 보안 취약점 이슈에 영향을 받는 제품 환경 하단의 제품 버전이거나 그보다 낮은 버전일 경우 Log4j 보안 취약점 이슈에 영향을 받는 제품이다.\n   Product Version     Tableau Server 2021.4, 2021.3.4, 2021.2.5, 2021.1.8, 2020.4.11, 2020.3.14, 2020.2.19, 2020.1.22, 2019.4.25, 2019.3.26, 2019.2.29, 2019.1.29, 2018.3.29   Tableau Desktop 2021.4, 2021.3.4, 2021.2.5, 2021.1.8, 2020.4.11, 2020.3.14, 2020.2.19, 2020.1.22, 2019.4.25, 2019.3.26, 2019.2.29, 2019.1.29, 2018.3.29   Tableau Prep Builder 2021.4.1, 2021.3.2, 2021.2.2, 2021.1.4, 2020.4.1, 2020.3.3, 2020.2.3, 2020.1.5, 2019.4.2, 2019.3.2, 2019.2.3, 2019.1.4, 2018.3.3   Tableau Public Desktop Client 2021.4   Tableau Reader 2021.4   Tableau Bridge 20214.21.1109.1748, 20213.21.1112.1434, 20212.21.0818.1843, 20211.21.0617.1133, 20204.21.0217.1203, 20203.20.0913.2112, 20202.20.0721.1350, 20201.20.0614.2321, 20194.20.0614.2307, 20193.20.0614.2306, 20192.19.0917.1648, 20191.19.0402.1911, 20183.19.0115.1143    필자의 경우, Log4j 보안 취약점 이슈가 해결된 패치 버전으로 업그레이드 하여 해당 문제를 조치하였으며 관련 내용은 다음과 같다.\n 방법1. 패치 버전 업그레이드  패치 버전 정리 2021.12.19 일자 패치 버전으로 업그레이드 하면, 현재 알려져 있는 CVE-2021-44228 \u0026amp; CVE-2021-45046 문제를 해결할 수 있다.\n   Product Version     Tableau Server 2021.4.2, 2021.3.6, 2021.2.7, 2021.1.10, 2020.4.13   Tableau Desktop 2021.4.2, 2021.3.6, 2021.2.7, 2021.1.10, 2020.4.13   Tableau Prep Builder 2021.4.3   Tableau Public Desktop Client 2021.4.2   Tableau Reader 2021.4.2   Tableau Bridge 20214.21.1109.1748    Tableau Server Upgrade  https://help.tableau.com/current/server/ko-kr/upgrade.htm\n Windows  2018.2 이상에서 업그레이드(Windows)  https://help.tableau.com/current/server/ko-kr/sug_plan.htm   2018.1 이하에서 업그레이드 수행(Windows)  https://help.tableau.com/current/server/ko-kr/sug_upgrade_samehrdwr_pretsm.htm     Linux  https://help.tableau.com/current/server-linux/ko-kr/upgrade.htm     필자는 Windows Server 2012 R2 환경에 설치되어있는 Tableau Server 2020.4.3 버전을 Tableau 2020.4.13 버전으로 업그레이드 하였다. 순서는 다음과 같다.\n설치 순서   Tableau Server 설치된 위치 확인\n  현재 Tableau Server 백업\n 서버 업그레이드 - Tableau Server 백업 - Tableau  tsm maintenance backup -f ts_backup -d      서버 업그레이드 설치 파일 다운로드\n 2020.4.13    단일 서버 업그레이드 설치 실행 (단일 서버 업그레이드 \u0026ndash; 설치 실행 - Tableau)\n 원격 서버 로그온 Tableau Server 설치 프로그램 복사한 폴더로 이동하여 설치 프로그램 실행 설치 프로그램에서 기존 버전의 위치가 표시되면 새 버전을 같은 위치에 설치 마지막 페이지에서 업그레이드가 아직 완료되는 메시지가 표시됨  업그레이드 스크립트 upgrade-tsm 실행하여 업그레이드 완료   Tableau Server 시작  TSM or 명령 프롬포트    Tableau Server 업그레이드 여부 확인      방법2. jndilookup.class 삭제   Tableau Server Mitigation Steps Tableau Desktop Mitigation Steps Tableau Prep Builder Mitigation Steps Tableau Bridge Mitigation Steps   Windows 또는 Linux 및 설치 환경에 따라 해당 링크에서 해결 방법을 확인할 수 있다. 해당 방법은 진행하지 않았기 때문에 관련 링크만 정리해두었다.\n 후기 Tableau Server 뿐 아니라 Desktop 및 Prep 등도 급하게 업데이트를 진행하였으나, 공식 홈페이지에 관련 내용이 잘 나와있으므로 별도로 정리하지는 않았다. 앞으로 이러한 치명적인 보안 이슈가 발생하지 않았으면 한다\u0026hellip;\n Reference  https://kb.tableau.com/articles/issue/Apache-Log4j2-vulnerability-Log4shell https://help.tableau.com/current/server/ko-kr/upgrade.htm https://help.tableau.com/current/server/ko-kr/sug_plan.htm https://help.tableau.com/current/server/ko-kr/sug_upgrade_samehrdwr_pretsm.htm https://help.tableau.com/current/server-linux/ko-kr/upgrade.htm  ","permalink":"https://zellyshu.github.io/posts/2021-12-23-tableau-log4j-issue/","summary":"Intro 최근 발생한 Log4j 보안 취약점 사태로 인하여 시끌시끌한 가운데, Tableau에서도 Log4j 보안 취약점에 대한 대응방안을 내놓았다. 따라서 이와 관련된 내용을 정리하고, 실제 업데이트를 해본 사항을 기록해둔다.\n Tableau에서의 Log4j Log4j Apache 라이브러리를 사용하는 Tableau 제품은 보안 취약점을 가지고 있으며, 이에 영향을 받는 제품들은 Log4j 보안 취약점 이슈가 해결된 패치 버전으로 업그레이드하거나, jndilookup.class를 삭제하여야 한다.\n https://kb.tableau.com/articles/issue/Apache-Log4j2-vulnerability-Log4shell\n  이슈 대응방안\n Log4j 보안 취약점 이슈가 해결된 패치 버전으로 업그레이드 jndilookup.","title":"Tableau Log4j 보안 이슈 대응 및 해결방안 정리"},{"content":"Intro 연말을 맞이하여 업무 및 일상에 있어 어떠한 툴과 프로그램들을 사용하고 있는지 현황을 파악해보아야겠다는 생각이 갑자기 들었다. 특히 올해는 생산성 향상이라는 방향에 있어 Product Hunt 및 뉴스레터 등에서 본 많은 툴들을 설치하고 제거하고를 수 없이 반복했기에 이걸 1년마다 최신화를 시킨다면 나중에 보았을 때도 괜찮은 자료로 남지 않을까 싶었다. 간단하게 테이블 형태로 정리해보니 생각보다 많은 양에 놀랐다. 혹여나 누군가에게 도움이 된다면 좋겠다는 생각에 정리 사항을 남겨본다.\n2021.12.15 기준    목적 Tool     프로젝트 관리 Notion -\u0026gt; Clickup   메일 클라이언트 Mozilla Thunderbird   Web Browser (업무) Chrome   Web Browser (개인) Firefox   Password Manager Bitwarden Premium   DB DBeaver   API Postman   Markdown Typora (유료화로 인하여 변경) -\u0026gt; Mark Text   SSH Putty -\u0026gt; MobaXterm   SCP WinSCP -\u0026gt; MobaXterm   Image Viewer 꿀뷰   영상편집 뱁믹스   압축파일 반디집   데이터 시각화 Tableau   Cloud Oracle, GCP   Python Coding IDE Pycharm   그 외 Coding IDE VSCode   개인 아카이브 Obsidian   생각 정리 Logseq   Workflow (mac OS) Alfred 4   Workflow (Win) Wox -\u0026gt; Fluent-Search   Clipboard Ditto   파일 및 폴더 검색 Everything   RSS Slack, Feedly   창 분할 (mac OS) Magnet   창 분할 (Win) PowerToys   VPN FortiClient VPN Only   노트 UpNote (Premium)   Font Pretendard   개인 일정 (여행 등) KanbanFlow   일정관리 (Phone) Calendars 5    보류    목적 Tool 보류 이유     프로젝트 관리 JetBrains Space 창이 두 개가 켜지지 않고, 스프린트 기반의 관리가 익숙해지지 않아서    ","permalink":"https://zellyshu.github.io/posts/2021-12-15-tool/","summary":"Intro 연말을 맞이하여 업무 및 일상에 있어 어떠한 툴과 프로그램들을 사용하고 있는지 현황을 파악해보아야겠다는 생각이 갑자기 들었다. 특히 올해는 생산성 향상이라는 방향에 있어 Product Hunt 및 뉴스레터 등에서 본 많은 툴들을 설치하고 제거하고를 수 없이 반복했기에 이걸 1년마다 최신화를 시킨다면 나중에 보았을 때도 괜찮은 자료로 남지 않을까 싶었다. 간단하게 테이블 형태로 정리해보니 생각보다 많은 양에 놀랐다. 혹여나 누군가에게 도움이 된다면 좋겠다는 생각에 정리 사항을 남겨본다.\n2021.12.15 기준    목적 Tool     프로젝트 관리 Notion -\u0026gt; Clickup   메일 클라이언트 Mozilla Thunderbird   Web Browser (업무) Chrome   Web Browser (개인) Firefox   Password Manager Bitwarden Premium   DB DBeaver   API Postman   Markdown Typora (유료화로 인하여 변경) -\u0026gt; Mark Text   SSH Putty -\u0026gt; MobaXterm   SCP WinSCP -\u0026gt; MobaXterm   Image Viewer 꿀뷰   영상편집 뱁믹스   압축파일 반디집   데이터 시각화 Tableau   Cloud Oracle, GCP   Python Coding IDE Pycharm   그 외 Coding IDE VSCode   개인 아카이브 Obsidian   생각 정리 Logseq   Workflow (mac OS) Alfred 4   Workflow (Win) Wox -\u0026gt; Fluent-Search   Clipboard Ditto   파일 및 폴더 검색 Everything   RSS Slack, Feedly   창 분할 (mac OS) Magnet   창 분할 (Win) PowerToys   VPN FortiClient VPN Only   노트 UpNote (Premium)   Font Pretendard   개인 일정 (여행 등) KanbanFlow   일정관리 (Phone) Calendars 5    보류    목적 Tool 보류 이유     프로젝트 관리 JetBrains Space 창이 두 개가 켜지지 않고, 스프린트 기반의 관리가 익숙해지지 않아서    ","title":"사용하는 툴 \u0026 프로그램 정리"},{"content":"Intro 최근 Logseq와 Obsidian을 같이 활용하여 생각을 정리해보고자 하고 있다. 회사에서는 Windows를 사용하지만, 평상시에는 mac OS 및 Apple 생태계의 기기들을 주로 사용하는 나에게 있어 크로스 플랫폼은 필수적인 요소인데 이를 만족하는 것이 Obsidian이었다. 최근 모바일 앱도 나와서 쏠쏠하게 사용 중에 있는데, 이를 활용하여 Logseq + Obsidian 함께 활용하기를 보고 Logseq를 통해서도 정리해보고자 마음을 먹게 된 것이다.\n그런데 글이 안보인다\u0026hellip;? icloud를 통하여 동기화시켜서 mac 환경과 windows 환경에서 모두 사용할 수 있게끔 세팅을 마쳤다. 그렇게 사용하고 있다가 mac에서 정리해둔 글을 Windows에서 확인하려 했는데 글이 보이지 않았다. 정확히는 [[]] 로 링크를 걸어둔 부분들이 보이지 않았다. 그래서 클라우드 업로드 내역을 확인해보니 해당 파일이 있다는 것은 확인할 수 있었다. 하지만 파일명이 전부 깨져있었다\u0026hellip;\n한글 자소분리 이슈 mac에서 email로 file 같은 걸 첨부해서 보낼 때마다 심심치 않게 파일명이 다 깨져서 오는 일이 있다. 한글자소분리 이슈때문인데 이와 마찬가지로 mac에서 작성한 글이 windows 환경에서 logseq에서 글이 보이지 않았던 이유는 mac에서 작성한 마크다운 파일이 클라우드에 자동 업로드 되면서 한글로 된 파일명이 자소분리가 되어 업로드되기 때문이었다.\n이를 해결해주는 프로그램을 감사히도 만들어주신 분이 있어 [Windows] 한글 자소 교정기 ver.2을 다운받아서 바로 파일명을 일괄 수정해주었다. 수정 이후 logseq에 들어가보니 잘 나오는 것을 확인할 수 있었다.\n후기 너무나 간단한 이유였지만 혹시나 누군가에게 도움이 될까 싶어 이슈사항을 정리해둔다.\nReference  https://namocom.tistory.com/630 https://luran.me/425  ","permalink":"https://zellyshu.github.io/posts/2021-11-12-logseq-hanguljaso/","summary":"Intro 최근 Logseq와 Obsidian을 같이 활용하여 생각을 정리해보고자 하고 있다. 회사에서는 Windows를 사용하지만, 평상시에는 mac OS 및 Apple 생태계의 기기들을 주로 사용하는 나에게 있어 크로스 플랫폼은 필수적인 요소인데 이를 만족하는 것이 Obsidian이었다. 최근 모바일 앱도 나와서 쏠쏠하게 사용 중에 있는데, 이를 활용하여 Logseq + Obsidian 함께 활용하기를 보고 Logseq를 통해서도 정리해보고자 마음을 먹게 된 것이다.\n그런데 글이 안보인다\u0026hellip;? icloud를 통하여 동기화시켜서 mac 환경과 windows 환경에서 모두 사용할 수 있게끔 세팅을 마쳤다.","title":"macOS에서 작성한 Logseq 내용이 Windows 환경에서 보이지 않을 때 (한글자소분리 이슈)"},{"content":"Intro Tableau를 업무 중에 계속 활용하다보니 어느정도 자신감이 붙어있는 상태였다. 하지만, 프로젝트를 진행하면서 발생하는 산출물과 같은 부분은 완전히 내 것으로 남는 것 같다는 느낌이 들지 않았다. 그러던 중 자격증명 같은 거라도 있으면 그래도 Tableau 관련 업무를 하며 남은 것이 생겼다라는 느낌이 들지 않을까 싶어 Tableau Desktop Specialist를 취득하고자 생각하게 되었다.\nTableau Desktop Specialist 기존에는 LES라는 시험 플랫폼을 통해서 시험을 진행했었지만, 지금은 Pearson VUE라는 플랫폼을 통해서만 시험을 진행하고 있다. 이전에 Tableau 관련 교육을 받을 때나 자격증 관련 이야기를 들을 때 LES에서 진행되는 형태만 들어왔기 때문에 당연히 그럴 줄 알고 준비를 진행하려 했었는데 막상 결제까지 하고보니 시험 방식이 달라졌다는 것을 그 때 알게 되었다\u0026hellip;\n이전에는 오픈북과 유사한 형태로 이론과 실습의 시험이 같이 진행되는 시스템이었지만, Pearson VUE로 바뀌면서 문제만 출제되는 형태로 바뀌었다. 이미 100$를 결제했기 때문에 돌이킬 순 없었고, 찾아봐도 Pearson VUE 환경에서 시험을 합격했다는 글을 찾기 어려워서 이럴거면 첫 스타트라도 끊어봐야겠다 싶어 (객기) 시험을 치루어보자란 마음을 먹게 되었다.\n공부 Tableau를 업무 중에도 활용하고 있기 때문에, Udemy 강의에서도 Test만 쳐보면 되겠지\u0026hellip;? 란 생각으로 Practice Test만 있는 Tableau Desktop Specialist Certification Practice Test 2021 를 선택하였다. 그리고 바로 시험을 치루었으나 결과는 처참했다\u0026hellip;\n그래서 일단 공식 홈페이지에서 지원하는 시험 준비 가이드를 정독하고, Tableau Desktop 프로그램을 켤 때마다 관련 메뉴나 선택 시 어떤 메뉴가 나오는지를 조금 더 유심히 보고 기억하고자 하였다. 그래도 관련 자료나 Error 발생 시 검색의 참고를 위하여 Tableau Desktop의 기본 언어를 영문으로 놓고 사용하고 있었는데 이러한 부분이 시험을 준비함에 있어 큰 도움이 되었다.\n그리고 Pearson VUE 환경으로 이루어지는 Tableau Desktop Specialist 시험의 정보를 습득하기 너무 어려웠기 때문에, (차라리 시험을 보신 분들의 후기라도 검색이 된다면 좋았을텐데\u0026hellip;) 일단 Udemy 강의의 Practice Test를 문제은행 답 외우듯이 열심히 외웠다. 물론, 오답이나 눈여겨보아야 할 2~3개 선택지의 경우에는 따로 정리하여 관련 이론이랑 같이 외웠다. 해당 강의의 경우 문제마다 해설도 덧붙여두어서 정리하며 이해하기에 좋았다.\nQuizlet도 검색해보니 Tableau Desktop Specialist 라는 자료가 있었는데 이전 LES 환경의 문제지만 이론 문제도 같이 있어서 이론 문제만 쭉 훑어보았고, 많은 도움이 되었다.\n그렇게 3~4회 정도 돌리고 시험 날을 맞이하게 되었다.\nPearson VUE 시험 환경을 굉장히 까다롭게 이야기하길래, 집에서 볼까 하다가 다른 사람이 캠에 잡히면 안되는 등 제약 조건이 많길래 아예 집 근처 스터디 카페의 세미나실을 빌려서 시험을 치뤘다. 30분 전부터 테스트를 진행하는데 신분 확인 목적으로 들고 갔던 여권이나 본인의 사진, 주변 환경을 찍어 전송하기도 하고, 네트워크와 스피커 및 마이크를 체크하기도 하다보니 30분이 빠듯하단 생각이 들었다. 켜져 있던 메신저나 다른 프로그램도 모두 off 하고 시험을 보기 시작하였다.\n중간에 하나 기억이 나는 일을 꼽자면, 무의식적으로 턱을 괴고 시험을 보고 있었는데 갑자기 chat이 오더니 손을 떼고 시험을 보라는 말을 감독관이 하였다. 그런 부분 말고는 이슈라고 할 만한 사항은 없었다.\n또한, 무선 환경에서 시험을 보다보니 혹시나 끊기지 않을까 걱정했는데 다행히 끊기지 않았다. 그래도 걱정이 될 부분이 많으니 무선 환경으로 볼 거라면 이용자가 비교적 적은 아침이나 밤이 괜찮지 않을까 덧붙여본다.\n결과 45문제를 풀고, 5개의 설문조사 비스무리한 문제를 풀고나면 바로 결과가 나온다. 결과는 다행히도 PASS 였다.\n750점을 넘으면 합격인데, 무사히 합격할 수 있었다. (score는 민망해서 가렸다.) 한국시간이랑 달라서 그런지 10월 30일에 보았는데 확인서에는 시험 날이 10월 29일로 나온다.\n후기 Udemy 강의나 Quizlet에 올라와 있는 자료들에 대해서 익숙해지고자 틈틈이 문제와 답을 외웠는데 막상 시험에 거의 그대로 나왔던 것은 Udemy 강의의 Practice 몇 개 정도였고, 나머지는 이해가 없으면 풀 수 없는 문제들이 주를 이루었다. (물론 비슷한 문제들도 나오긴 했지만) 이론을 같이 공부하지 않았다면 합격하기 어렵지 않았을까 싶다. 그래도, Tableau를 자주 활용한다면 완전히 처음 보는 수준의 문제라고 보기는 어렵고, 마우스로 움직여 만들던 것을 글로 풀어 설명해 놓으니 낯설다는 느낌이 강했다. 아주 높은 수준으로 합격을 한 것은 아니지만, 혹시나 Pearson VUE 환경에서 시험을 보실 분들에게 조금이나마 도움이 되길 바라는 마음에서 후기를 남겨보았다. 모두 화이팅 !\nReference  https://www.tableau.com/ko-kr/learn/certification/desktop-specialist https://home.pearsonvue.com/tableau https://www.udemy.com/course/tableaudesktop/ https://quizlet.com/557554674/tableau-desktop-specialist-flash-cards/  ","permalink":"https://zellyshu.github.io/posts/2021-10-30-tableau-desktop-specialist-review/","summary":"Intro Tableau를 업무 중에 계속 활용하다보니 어느정도 자신감이 붙어있는 상태였다. 하지만, 프로젝트를 진행하면서 발생하는 산출물과 같은 부분은 완전히 내 것으로 남는 것 같다는 느낌이 들지 않았다. 그러던 중 자격증명 같은 거라도 있으면 그래도 Tableau 관련 업무를 하며 남은 것이 생겼다라는 느낌이 들지 않을까 싶어 Tableau Desktop Specialist를 취득하고자 생각하게 되었다.\nTableau Desktop Specialist 기존에는 LES라는 시험 플랫폼을 통해서 시험을 진행했었지만, 지금은 Pearson VUE라는 플랫폼을 통해서만 시험을 진행하고 있다. 이전에 Tableau 관련 교육을 받을 때나 자격증 관련 이야기를 들을 때 LES에서 진행되는 형태만 들어왔기 때문에 당연히 그럴 줄 알고 준비를 진행하려 했었는데 막상 결제까지 하고보니 시험 방식이 달라졌다는 것을 그 때 알게 되었다\u0026hellip;","title":"Tableau Desktop Specialist 합격 후기 (Pearson VUE 환경)"},{"content":"Intro RSS 구독을 통하여 다양한 기술 블로그 및 회사들의 글을 읽으며 아침 일과를 시작하는 편이다. 그러던 중, 뱅크샐러드 기술 블로그에서 '# 뱅크샐러드 Data Discovery Platform의 시작' 이라는 글을 읽게 되었는데 갑자기 흥미가 생겨서 데이터 디스커버리 툴에 대하여 찾아보게 되었다.\nData Discovery Tool? Data Discovery Tool이란, 말 그대로 데이터의 탐색을 돕는 툴로서 여러 소스에서 데이터를 수집 및 결합하고 데이터의 패턴과 추세를 식별하는 데 도움이 되는 도구를 의미한다. BI 툴에서 많이 찾아볼 수 있다. 특히, 데이터를 다루는 많은 회사들이 Data Discovery Tool들을 사용하고 있으며, Linkedin의 Data Hub, Netflix의 Metacat, Lyft의 Amundsen 등 오픈소스로 제공하는 회사들도 많다.\nAmundsen 처음에는 Linkedin의 Data Hub를 간단하게 설치해보려 했으나, 베이글코드라는 회사에서 사용하는 사내 디스커버리 툴인 Amundsen을 어떻게 도입하게 되었는지를 다루는 # Data Discovery Platform at Bagelcode 라는 글을 읽게 되면서 Python 기반으로 이루어진 Amundsen을 설치해보자 라는 생각이 더 강해지게 되었다. (구글 검색을 했을 때, DataHub에 관한 자료탐색 시 DataHub라는 이름이 어떻게 보면 너무 흔한 이름이다 보니 Linkedin의 DataHub라는 툴에 대한 내용이 찾기 어려웠던 점은 덤)\nInstallation 설치 선행 조건은 다음과 같다.\n  Python = 3.6 or 3.7 Node = v10 or v12 (v14 may have compatibility issues) npm \u0026gt;= 6   설치 방법은 Docker로 설치하면 되는데, Backend를 Neo4j로 하느냐, Atlas로 하느냐에 따라 2가지로 나뉘게 된다.\n 적어도 3GB의 Docker 여유 용량이 있는지 확인할 것 Git Clone $ git clone --recursive https://github.com/amundsen-io/amundsen.git ``\n Backend 선택 # For Neo4j Backend $ docker-compose -f docker-amundsen.yml up # For Atlas $ docker-compose -f docker-amundsen-atlas.yml up ``\n Sample data 설치 $ python3 -m venv venv $ source venv/bin/activate $ pip3 install --upgrade pip $ pip3 install -r requirements.txt $ python3 setup.py install $ python3 example/scripts/sample_data_loader.py ``\n http://localhost:5000 접속  test 검색 후 결과 확인  검색해보면, 하단의 이미지와 같이 나오게 된다. 설치 중 에러 발생했던 내용 정리 ERROR: Service \u0026lsquo;amundsenfrontend\u0026rsquo; failed to build : Build failed ~  Atlas를 통한 설치 시에 발생한 에러. 권한 관련 이슈로 보이는데 결국 잡지 못하고 neo4j를 통한 설치 방법으로 우회  error in amundsen-databuilder setup command: \u0026lsquo;extras_require\u0026rsquo; must be a dictionary whose values are strings or lists of strings containing valid project/version requirement specifiers.  github 내 Issue에 관련 사항이 있었음. amundsen\\databuilder 폴더 내의 requirements-dev.txt 의 내용을 amunsen 폴더의 requirements-dev.txt의 내용으로 변경하면 된다. 확인해보니 경로 관련 이슈가 있었던 것으로 보인다. 참고 : https://github.com/amundsen-io/amundsen/issues/1447  후기 설치 과정은 중간에 에러만 발생하지 않았다면, 상당히 쉬운 편이었다. 앞으로는 Amundsen을 활용하여 여기저기 내 컴퓨터 및 클라우드에 흩어져 있는 데이터를 효율적으로, 체계적으로 관리해보려 노력해야겠다.\nReference  https://blog.banksalad.com/tech/the-starting-of-datadiscoveryplatform-era-in-banksalad/ https://medium.com/bagelcode/data-discovery-platform-at-bagelcode-b58a622d17fd https://github.com/amundsen-io/amundsen/ https://www.softwareadvice.com/bi/data-discovery-tools-comparison/  ","permalink":"https://zellyshu.github.io/posts/2021-10-19-docker-amundsen-installation/","summary":"Intro RSS 구독을 통하여 다양한 기술 블로그 및 회사들의 글을 읽으며 아침 일과를 시작하는 편이다. 그러던 중, 뱅크샐러드 기술 블로그에서 '# 뱅크샐러드 Data Discovery Platform의 시작' 이라는 글을 읽게 되었는데 갑자기 흥미가 생겨서 데이터 디스커버리 툴에 대하여 찾아보게 되었다.\nData Discovery Tool? Data Discovery Tool이란, 말 그대로 데이터의 탐색을 돕는 툴로서 여러 소스에서 데이터를 수집 및 결합하고 데이터의 패턴과 추세를 식별하는 데 도움이 되는 도구를 의미한다. BI 툴에서 많이 찾아볼 수 있다.","title":"Amundsen을 Docker에 설치해보기"},{"content":"Intro OpenVSCode Server를 띄워두고 태블릿 PC등으로 외부접속하면 테스트 목적으로 간단하게 사용할 수 있지 않을까? 라는 생각이 갑자기 들어 Docker 환경에서 우선 설치해보기로 하였다.\nOpenVSCode Server란? OpenVSCode Server란 원격 컴퓨터에서 서버를 실행하고, 웹 브라우저를 통해 접근할 수 있는 VSCode 버전을 제공하는 것으로, Micorsoft사에서 오픈소스 프로젝트로 개발하고 있다.\nInstallation 설치는 Docker나 Linux 환경에서 설치 가능하며, Docker 환경에서 설치를 진행하였다. 사실 명령어 하나만 입력하면 설치가 진행되므로, 오류가 안나기만을 간절히 기도하였다\u0026hellip;\ndocker run -it --init -p 3000:3000 -v \u0026#34;$(pwd):/home/workspace:cached\u0026#34; gitpod/openvscode-server 설치 및 설정이 완료되면 3000번 포트를 통해 접속할 수 있다.\n접속하면, 테마 등을 커스터마이징 할 수 있다. Dracula 테마를 검색하여 적용하였더니 다음과 같이 잘 나오는 것을 확인할 수 있었다.\n후기 기존 컴퓨터에 설치되어 있는 VSCode의 익스텐션들을 이것저것 OpenVSCode Server 환경에서도 설치해보았다. VSCode를 사용하는 것처럼 위화감과 같은 불편함은 느껴지지 않았다. 다만, Python의 경우 설치 과정 중 접근 권한이 꼬이는지 설치가 자꾸 에러가 났다. 차후 Linux 환경에서 한 번 더 차근차근 설정해보아야 할 것 같다\u0026hellip;\nReference  https://github.com/gitpod-io/openvscode-server https://www.gitpod.io/blog/openvscode-server-launch  ","permalink":"https://zellyshu.github.io/posts/2021-10-12-openvscode-server-docker/","summary":"Intro OpenVSCode Server를 띄워두고 태블릿 PC등으로 외부접속하면 테스트 목적으로 간단하게 사용할 수 있지 않을까? 라는 생각이 갑자기 들어 Docker 환경에서 우선 설치해보기로 하였다.\nOpenVSCode Server란? OpenVSCode Server란 원격 컴퓨터에서 서버를 실행하고, 웹 브라우저를 통해 접근할 수 있는 VSCode 버전을 제공하는 것으로, Micorsoft사에서 오픈소스 프로젝트로 개발하고 있다.\nInstallation 설치는 Docker나 Linux 환경에서 설치 가능하며, Docker 환경에서 설치를 진행하였다. 사실 명령어 하나만 입력하면 설치가 진행되므로, 오류가 안나기만을 간절히 기도하였다\u0026hellip;\ndocker run -it --init -p 3000:3000 -v \u0026#34;$(pwd):/home/workspace:cached\u0026#34; gitpod/openvscode-server 설치 및 설정이 완료되면 3000번 포트를 통해 접속할 수 있다.","title":"OpenVSCode Server를 Docker에 설치해보기"},{"content":"Intro 토이 프로젝트를 진행할 때 테스트 용도로 DB를 불러올 일이 종종 생겼다. 토이 프로젝트이기 때문에 정말 기본만 간단하게 구성된 DB와 연결해 몇가지 테스트만 해보면 될 상황에도 그 때 그 때 DB를 만들고 하는 건 여간 귀찮은 일이 아니었다. 그래서 최대한 무료 범위 내에서 해결하고자 알아보았고, Heroku로도 테스트 목적에는 손색이 없을 것이라 생각하여 바로 실행에 옮겼다.\nDB 생성 신용카드를 등록해두면, 월 1000시간 / 500 MB 제한으로 무료 사용할 수 있다. 또한, MariaDB만 하나 구축해두려다가 PostgreSQL도 하나 더 구축해두었다. 사실 \u0026lsquo;구축\u0026rsquo; 이라고 하기에도 너무 민망한 것이 버튼 몇 번으로 설정이 끝나기 때문이다. 기억을 기록해두기 위해 적는 것이라는 점 알아주셨으면 좋겠다\u0026hellip;\nHeroku 를 통한 무료 MariaDB (JawsDB) 생성   \u0026lsquo;Create New app\u0026rsquo; 으로 새로운 app을 생성한 뒤,\n  \u0026lsquo;Resources\u0026rsquo; 메뉴에서 JawsDB Maria 라는 Add-ons 를 추가하면 끝\n Billing에서 신용카드를 등록해야 함 Kitefin Shared Plan의 경우만 Free이며, 사양은 다음과 같다.    속성 내용     Direct SQL Access ✅   Connections 10   RAM Shared   Storage Capacity 5 MB   Daily Backups ✅   Backup Retention 1 Day   Databites ✅        Settings에서 DB 관련 정보를 확인할 수 있다.\n  Common Runtime과 Private Spaces 비교\n Common Runtime     Private Spaces        Heroku 를 통한 무료 PostgreSQL 생성   \u0026lsquo;Create New app\u0026rsquo; 으로 새로운 app을 생성한 뒤,\n  \u0026lsquo;Resources\u0026rsquo; 메뉴에서 Heroku Postgres 라는 Add-ons 를 추가하면 끝\n Billing에서 신용카드를 등록해야 함 Hobby Dev Plan의 경우만 Free이며, 사양은 다음과 같다.    속성 내용     Postgres Extensions ✅   RAM 0 Bytes   Direct SQL access ✅   Row Limit 10,000   Storage Capacity 1 GB   Dataclips ✅   Connection Limit 20   Rollback 0 Seconds   PostGIS ✅   PGBackups ✅   PGBackups Retained 2 Backups        Settings에서 DB 관련 정보를 확인할 수 있다.\n  Common Runtime과 Private Spaces 비교\n Common Runtime     Private Spaces        DBeaver 연결 평소 DB Tool로 DBeaver를 사용하고 있으므로, DBeaver에 연결하여 불러봤더니 잘 나오고 있으므로 외부 연결도 무리 없음을 확인하였다.\n후기 DBeaver에서도 접근이 가능하기 때문에 테스트 용도로서의 DB를 만들어두자 라는 목적은 충실하게 이루었다고 볼 수 있을 것 같다. 또한, 몇 번의 버튼 클릭만으로 설정이 완료되기 때문에 매우 간편하게 언제든 (물론 Heroku의 무료 정책상 제약은 있지만) 접근 가능한 DB를 만들 수 있다는 점은 강력한 메리트라 생각한다.\nReference  https://www.heroku.com/ https://zeddios.tistory.com/1233  ","permalink":"https://zellyshu.github.io/posts/2021-09-29-heroku-mariadb-postgresql/","summary":"Intro 토이 프로젝트를 진행할 때 테스트 용도로 DB를 불러올 일이 종종 생겼다. 토이 프로젝트이기 때문에 정말 기본만 간단하게 구성된 DB와 연결해 몇가지 테스트만 해보면 될 상황에도 그 때 그 때 DB를 만들고 하는 건 여간 귀찮은 일이 아니었다. 그래서 최대한 무료 범위 내에서 해결하고자 알아보았고, Heroku로도 테스트 목적에는 손색이 없을 것이라 생각하여 바로 실행에 옮겼다.\nDB 생성 신용카드를 등록해두면, 월 1000시간 / 500 MB 제한으로 무료 사용할 수 있다. 또한, MariaDB만 하나 구축해두려다가 PostgreSQL도 하나 더 구축해두었다.","title":"Heroku를 통한 무료 MariaDB \u0026 PostgreSQL 저장소 만들기"},{"content":"Intro WSL2를 통해 Ubuntu를 설치한 뒤 Windows 환경이라는 제약하에 못해 본 것들을 하나 둘 해보던 와중 Apache Superset이라는 오픈소스 시각화 BI툴을 설치하지 못했던 기억이 났다. Mac이나 Linux에서밖에 설치할 수 없어 Windows 환경에서는 VM 등의 가상머신을 이용할 수밖에 없었던 것으로 기억하고 있으나, WSL2의 지원 이후 Docker를 통해 설치가 가능하다는 것을 알게 되었다. 어차피 Docker도 설치해볼 겸 겸사겸사 Docker Desktop의 설치와 Apache Superset의 설치를 진행하게 되었다.\nDocker 유료화 관련 정책으로 요근래 다시 시끌시끌해진 Docker를 오랜만에 다시 설치해보았다. Windows에서도 이젠 설치가 무사히(?) 된다는 것에 또 한 번 놀랐다. Docker 홈페이지에서 다운로드 한 뒤 .exe 파일을 통하여 설치하면 끝이다. Docker Image만 있으면 손쉽게 같은 환경을 구성할 수 있다는 건 다시 봐도 의존성 오류 때문에 몇 날 며칠을 머리 싸매고 고뇌하던 내게 너무나도 큰 혁신이었던 기억이 난다.\n절차에 따라 설치를 마무리하면 튜토리얼 비스무리한게 나온다. 이제 Apache Superset을 설치할 시간이다.\nApache Superset Apache Superset 홈페이지에서 Apache Superset에 대하여 \u0026ldquo;Apache Superset is a modern data exploration and visualization platform\u0026rdquo; 이라 설명하고 있다. 회사 업무로 인하여, Tableau를 자주 사용하고 있는 나에게 있어 Apache Superset은 낯설지 않은 느낌으로 다가오는 BI 툴이었다. 실제로 여러 곳에서 Tableau의 대체재로 Apache Superset을 많이 꼽는 편이다. 연결할 수 있는 DB의 종류도 대표적인 DB라 할 수 있는 Oracle, MySQL, MSSQL 뿐만 아니라 Amazon REDSHIFT, Google BigQuery, IBM DB2 등도 지원하고 있어 오픈소스임에도 상용 DB까지 폭 넓게 지원하고 있다는 장점이 있다.\nApache Superset Installation WSL2의 Docker 환경에서 설치하였다. 설치 방법은 다음과 같다.\n  git clone\n$ git clone https://github.com/apache/superset.git ``\n  Docker를 통한 apache superset 컨테이터 실행\n$ cd superset $ docker-compose -f docker-compose-non-dev.yml up ``\n  하지만 설치 도중 (역시나\u0026hellip;?) 에러가 발생하였다.\n찾아보니 Windows OS의 Docker 환경에서 설치할 때 발생하는 에러로 보였고, 다음과 같이 해결할 수 있었다.\nsuperset_worker exited with code 127 Error 해결 docker pull preset/superset docker run -d -p 8080:8080 --name superset preset/superset docker exec -it superset superset fab create-admin --username admin --firstname Superset --lastname Admin --email [admin@superset.com](mailto:admin@superset.com) --password admin docker exec -it superset superset db upgrade docker exec -it superset superset load_examples docker exec -it superset superset init Error Reference\n https://github.com/apache/superset/issues/8632#issuecomment-630079535  Test 설치가 완료되면, Docker에서 관련 리소스를 확인할 수 있다. 한 눈에 사용량 등을 파악할 수 있어 관리 및 운영 시 용이하다.\n그리고, 8080번 포트(default)에 접속하면, Apache Superset에 접근할 수 있다.\n http://127.0.0.1:8080\n 해당 url로 접속하여 위에서 설정한 ID와 PW를 입력하면, Dashboard 메뉴로 이동한다.\n위의 이미지와 같이 시각화 예제 리스트가 있으며, 클릭해보면 출력되는 내용들을 확인할 수 있다.\n예제 화면을 보면, Tableau에서 Dashboard 화면을 구성하듯 Apache Superset에서도 유사하게 Dashboard를 구성할 수 있을 것으로 보인다. Tableau에서보다 간단하게 구성할 수 있을 것 같기도 하다. 어떤 Tool이 익숙하냐의 차이긴 하겠다만\u0026hellip;\n후기 Tableau를 통하여 업무를 진행하다보면 Power BI와 같이 자주 비교되는 대상이 바로 Apache Superset인데, 매번 문서를 통한 비교내역만 보다가 실제로 설치하여 구성해보니 비교점을 확실히 눈으로 파악할 수 있어 좋았다. 개인적으로, DB및 개발쪽 관련 지식이 있으신 분들이 Apache Superset가 Tableau보다 친숙한 느낌을 받을 것 같다는 생각이 들었다. 또한, 오픈소스는 언제나 관심을 가지고 있는 영역이기 때문에 시각화 BI 오픈소스 툴이 어떤 모습인지 파악해보는 것도 좋은 공부가 되었다. 이왕 설치하여 환경을 구성해놓았으니, Apache Superset을 활용할 방법에 대하여 계속 모색해 보아야겠다.\nReference  https://www.lainyzine.com/ko/article/a-complete-guide-to-how-to-install-docker-desktop-on-windows-10/ https://github.com/apache/superset https://superset.apache.org/docs/installation/installing-superset-using-docker-compose  ","permalink":"https://zellyshu.github.io/posts/2021-09-23-docker-apache-superset-installation/","summary":"Intro WSL2를 통해 Ubuntu를 설치한 뒤 Windows 환경이라는 제약하에 못해 본 것들을 하나 둘 해보던 와중 Apache Superset이라는 오픈소스 시각화 BI툴을 설치하지 못했던 기억이 났다. Mac이나 Linux에서밖에 설치할 수 없어 Windows 환경에서는 VM 등의 가상머신을 이용할 수밖에 없었던 것으로 기억하고 있으나, WSL2의 지원 이후 Docker를 통해 설치가 가능하다는 것을 알게 되었다. 어차피 Docker도 설치해볼 겸 겸사겸사 Docker Desktop의 설치와 Apache Superset의 설치를 진행하게 되었다.\nDocker 유료화 관련 정책으로 요근래 다시 시끌시끌해진 Docker를 오랜만에 다시 설치해보았다.","title":"WSL2 (Win10) Ubuntu20.04에서 Docker Desktop 설치하고 Apache Superset 테스트해보기"},{"content":"Intro 회사에서 Tableau를 사용하여 프로젝트를 진행할 일이 생기게 되면서 Tableau라는 툴에 대한 관심이 많아졌다. 데이터 시각화의 경우 D3.js와 Phthon의 라이브러리인 Seaborn 과 Matplotlib 정도만 사용해보았기에 Tableau에 적응하는 것은 매우 신선한(?) 작업이었다.\nTabpy 하지만, Tableau 프로그램에서 자체적으로 지원하는 분석 모델 및 예측 모델은 한정적이고, 속도에 대한 이슈도 존재했기 때문에 다른 방안을 알아보던 중 TabPy라는 분석 확장 프로그램을 찾게 되었다. Tableau 홈페이지에서는 TabPy에 대하여 데이터를 불러오기 전 전처리 작업이나 분석 및 예측 모델 등에 활용할 수 있다고 설명하고 있다. TabPy의 설치는 다음과 같은 환경에서 설치를 진행하였고, 사양은 다음과 같다.\n설치 환경 및 사양  Windows 10 Pro Python 3.8 (conda env) Tabpy == 2.4.0 Tableau Desktop 2020.4  Installation 설치 방법은 다음의 절차로 진행하면 된다.\n1. pip upgrade python -m pip install --upgrade pip 2. tabpy install pip install tabpy 3. tabpy 실행 tabpy 4. tabpy 실행체크  http://localhost:9004/\n 설치 완료 후 localhost의 9004번 포트에 접속해보면 TabPy 관련 화면이 출력되는 것을 확인할 수 있다.\nTest TabPy를 활용하기에 앞서, Tableau Desktop과 TabPy Server를 연결시켜주어야 한다. TabPy 서버의 연결은 Tableau Desktop에서 \u0026lsquo;도움말 \u0026gt; 설정 및 성능 \u0026gt; Analytics 확장 프로그램 연결 관리\u0026rsquo; 메뉴에서 연결해주면 된다. 연결이 성공하면 다음과 같은 화면이 출력된다.\n분석 기본으로 4가지의 분석 모델을 제공하고 있으며 관련 패키지는 다음의 명령어로 설치할 수 있다.\ntabpy-deploy-models 4가지의 분석 모델은 다음과 같으며, 설치가 완료되면 9004번 포트의 화면에도 설치된 분석 모델이 출력되는 것을 확인할 수 있다.\n PCA (Principal Component Anaylsis) Sentiment Analysis T-Test ANOVA  예제 1 Sample - Superstore 활용예제 (np.corrcoef) Tableau의 샘플 데이터셋인 Sueprstore 데이터에서 Numpy의 np.corrcoef 함수를 활용하는 예제이다. 테이블 계산을 Customer Name 기준으로 해서, Region별 Sales의 합과 Sub-Category별 Profit의 합 간 상관관계를 분석한다.\nSCRIPT_REAL(\u0026#39; import numpy as np return np.corrcoef(_arg1,_arg2)[0,1] \u0026#39;, SUM([Sales]), SUM([Profit])) 계산된 필드를 위와 같이 만들어 계산된 필드를 색상 범례로 추가하는 방식으로 활용할 수 있다.\n예제 2 Airbnb Data 활용예제 Airbnb 데이터셋을 사용하는 예제로, TabPy를 활용하여 클러스터링 기준을 생성하는 예제이다.\n우선 Airbnb 데이터셋을 설치하여 Tableau에서 불러온다.\n시트로 이동한 뒤, 클러스터링 범위를 조절할 때 사용하기 위하여 다음과 같이 Parameter를 생성한다.\n또한, 클러스터링 알고리즘을 변경할 때 사용하기 위하여 다음과 같이 Parameter를 추가 생성한다.\n계산된 필드를 하단과 같이 생성한다.\nSTR(SCRIPT_REAL(\u0026#34; import numpy as np import numpy.ma as ma from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans, MiniBatchKMeans, AffinityPropagation print(\u0026#39;Start\u0026#39;) # Scaling Features sc= StandardScaler() avg_price = sc.fit_transform(np.array(_arg1).reshape(-1,1)) med_review = sc.fit_transform(np.array(_arg2).reshape(-1,1)) med_beds = sc.fit_transform(np.array(_arg3).reshape(-1,1)) n_cl = _arg4[0] # Combine Scaled feature X_comb = np.column_stack([avg_price, med_review, med_beds]) # Handling null value with masked array X = np.where(np.isnan(X_comb), ma.array(X_comb, mask=np.isnan(X_comb)).mean(axis=0), X_comb) # Modeling result = [] if _arg5[0]==1: kmeans = KMeans(n_clusters = n_cl, random_state=99) result = kmeans.fit_predict(X).tolist() elif _arg5[0]==2: minib = MiniBatchKMeans(n_clusters = n_cl, random_state=99) result = minib.fit_predict(X).tolist() else: aff = AffinityPropagation().fit(X) result = aff.predict(X).tolist() return result \u0026#34;, AVG([Price]), MEDIAN([Review Scores Rating]), MEDIAN([Beds]), [Cluster Numbers], [Clustering Algorithm] )) Zipcode 기준으로 시각화를 진행한다.\n이후 위에서 생성한 Calculated Field를 색상 범례에 넣어서 클러스터링 기준으로 삼으면 다음과 같은 화면이 출력되는 것을 확인할 수 있다. 하단의 gif 이미지처럼 앞서 생성한 Parameter를 통하여 클러스터링 갯수와 알고리즘을 손쉽게 변경할 수 있다.\n후기 TabPy를 활용해본 결과 Tableau의 필터와 계산된 필드만을 통해 계산을 진행할 때보다 훨씬 분석이 용이하다는 느낌을 받았다. 기존에 Python을 활용하여 데이터를 분석한 경험이 있는 사람이라면, TabPy를 활용하는 것이 오히려 더 편할 것이라 생각된다. Python과 더불어 통계 분석에 많이 활용되는 R의 경우에도 TabPy처럼 Rserve를 활용하여 서버의 형태로 연결하여 이용할 수 있는 것으로 보인다. 예제 위주로 Test만 진행해 본 것이기 때문에 많은 내용을 살펴보지는 못했으나 활용 방안은 상황에 따라 무궁무진할 것으로 예상된다.\n활용 방안 아이디어 다음의 상황에서 TabPy를 활용할 수 있지 않을까 싶어 메모를 기록해둔다.\n 유사도 측정 시 여러 개의 feature를 하나로 합쳐 나타내야 할 때 예측 모델을 Python에서 가져와 계산에 활용 머신러닝으로 긍/부정 같이 이진 분류 후 시각화의 기준(Detail)으로 추가  Reference  https://tableau.github.io/TabPy/ http://alexloth.com/tabpy-tutorial-integrating-python-tableau-advanced-analytics/ https://www.kdnuggets.com/2020/11/tabpy-combining-python-tableau.html https://help.tableau.com/current/pro/desktop/ko-kr/functions_functions_tablecalculation.htm  ","permalink":"https://zellyshu.github.io/posts/2021-09-14-tabpy-intro/","summary":"Intro 회사에서 Tableau를 사용하여 프로젝트를 진행할 일이 생기게 되면서 Tableau라는 툴에 대한 관심이 많아졌다. 데이터 시각화의 경우 D3.js와 Phthon의 라이브러리인 Seaborn 과 Matplotlib 정도만 사용해보았기에 Tableau에 적응하는 것은 매우 신선한(?) 작업이었다.\nTabpy 하지만, Tableau 프로그램에서 자체적으로 지원하는 분석 모델 및 예측 모델은 한정적이고, 속도에 대한 이슈도 존재했기 때문에 다른 방안을 알아보던 중 TabPy라는 분석 확장 프로그램을 찾게 되었다. Tableau 홈페이지에서는 TabPy에 대하여 데이터를 불러오기 전 전처리 작업이나 분석 및 예측 모델 등에 활용할 수 있다고 설명하고 있다.","title":"TabPy 설치 테스트"},{"content":"Intro 본 포스팅은 Python의 한국어 형태소 분석 라이브러리인 konlpy를 활용하여 CentOS 7 + Flask + gunicorn + Nginx 조합으로 웹 애플리케이션 서비스를 만들어본 토이 프로젝트 개발기다. 개발된 서비스는 링크(클릭)를 통하여 이동하면 사용할 수 있으나, 이용자에게 제공되는 완전하고 안정된 형태의 서비스라기보다 Oracle Cloud의 무료 티어를 활용해보기 위한 테스트 목적이 강하므로 이 점을 인지하여 준다면 감사할 것 같다. (갑자기 다운되거나 오류 발생 확률 매우매우매우 높음ㅠㅠ)\n왜 만들었나? 취직하여 일을 시작한 지 얼마 되지 않았을 무렵, 사내 팀에서 AWS를 사용하고 있어 자연스레 클라우드 환경에 관심이 많아진 상태였다. 물론, 대학원생 시절 AWS와 GCP를 사용해본 적은 있으나 (딥러닝 모델 학습하다가 한달에 몇백만원 찍히고 놀래서 서버 다 내린 건 언젠가 풀 또 하나의 이야기\u0026hellip;) 완전히 친숙하다고 이야기하기엔 아주 애매한 사이였고, 클라우드 환경 하에서 웹 애플리케이션 서비스를 하나 만들어 배포해보면 지금보다는 더 익숙해지지 않을까 라는 결론에 자연스레 이르렀다.\nAWS와 GCP 중 어떤 걸 선택하여 테스트해볼까 고민하던 중, Oracle Cloud의 무료 티어가 눈에 들어오게 되었고 생각보다 괜찮은 사양이었기 때문에 이왕 해보는 거 기존에 사용해본 적 없었던 것을 사용해보자라는 마음으로 Oracle Cloud를 선택하게 되었다.\n처음 목적은 클라우드 환경과 친숙해지기 + flask와 gunicorn 조합을 활용해보기 였기 때문에 석사과정 때 몇 날 며칠을 밤새 만들었던 \u0026lsquo;한국어 형태소 분석기\u0026rsquo; 의 데모 서비스를 그대로 올려 사용하려 했다. 하지만 무료 티어의 한계로 인해(ㅠㅠ) 선학습된 tensorflow 모델을 올릴 때 memory out이 발생하는 등의 자원부족으로 인한 오류가 발생하였고, 대체할 만한 가벼운 서비스를 빠르게 만들어보아야 겠다고 생각하게 되었다.\n무언가를 입력하면 어떠한 결과가 출력되는 형태의 서비스를 생각하다보니, konlpy에서 지원하는 형태소 분석기마다의 모든 결과를 뿌릴 수 있도록 하는 웹페이지를 만들면 어떨까 하는 생각이 들었다. 화면도 조금만 수정하면 될 것 같아서 빠르게 실행에 옮겼다.\nCentOS 7 + Flask + gunicorn + Nginx OS는 CentOS 7로 결정하고, Flask와 gunicorn 조합을 활용해보기로 마음 먹었다. 아래는 간략한 설명(을 가장한 사족)이다.\nCentOS 7 CentOS 8로 해볼까 하다가 지원 관련 이슈가 괜히 신경쓰여서 7로 선택했다. 선택해 놓고 보니 CentOS 지원 종료 이슈가 계속 마음에 걸려 언젠가는 Rocky Linux로 변경해볼까 하는 생각이 계속 든다\u0026hellip;\nFlask Flask는 Django와 달리 매우매우 가볍다. 또한, 붙이고 싶은 대로 이것저것 붙이는 것이 자유로운 웹 프레임워크다. Tensorflow로 선학습된 모델을 올려 가볍게 테스트용 서비스를 만들 때 자주 사용했었다. 하지만, 너무 자유로워서 이게 왜 돌아가는 거지 싶을 때가 종종 있었다\u0026hellip;\ngunicorn 기존에는 uwsgi를 자주 사용했었고, 이번에도 사용하려 하였다. 하지만, uwsgi가 자원을 많이 소모한다는 정보를 어디선가 들었고, 테스트의 목적이라면 전혀 사용해본 적이 없는 것을 사용해보아야 진정한 테스트가 아니겠는가라는 이유로 gunicorn을 활용하게 되었다. 실제로 적용해보고 나니 굉장히 쉽고 빨라서 다른 개발에도 사용하면 괜찮겠다란 생각이 들었다.\nNginx Apache는 사내에서도 사용해 볼 일이 있었기 때문에 스터디 목적으로라도 Nginx를 적용해보아야 겠다는 생각을 했고, 실제로 적용해보니 굉장히 가볍고 빨랐다. 최대한 빠르게 만들어서 테스트해야지라는 목적에도 잘 부합하는 선택이었다고 생각한다.\n이외의 설정들 FrontEnd 이전에 bootstrap 기반의 테마를 다운받아 적당히 커스텀했던 것을 가져와서 사용하였다. 당시 Frontend에 대한 경험이 너무 적었고, (물론 지금이라고 더 낫다고 할 순 없지만\u0026hellip;) 물어볼 사람도 너무나 적었기 때문에 하나하나 뜯어가며 하드코딩했던 기억이 난다. 당시에도 반응형으로 만들고 싶었지만, 모바일을 비롯 해상도에 따라 깨져 보이는 것은 본인도 알고 있으며(ㅠㅠㅠ), 관련 분야 실력이 부족한 것을 한탄할 뿐이다.\nURL 접근은 \u0026lsquo;내도메인한국\u0026lsquo;에서 별도의 URL을 생성해서 redirect 해주는 형태로 쉽게 접근할 수 있도록 하였다. 실제 서비스가 아니기 때문에 SSL 같은 요소는 따로 적용하지 않았다.\n설치 python 및 nginx 를 설치한다.\nsudo yum install python-pip python-devel gcc nginx Konlpy 라이브러리를 사용하기 위한 선행 설치 요건들을 설치해준다.\nsudo yum install gcc-c++ java-1.8.0-openjdk-devel sudo yum install curl git bash \u0026lt;(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh) 설치가 완료되면, pip를 통하여 flask와 gunicorn을 설치해준다.\n관리가 용이하도록 virtualenv 모듈을 통하여 가상환경을 만들어 설치해주었다.\nvirtualenv venv source venv/bin/activate pip install gunicorn flask konlpy 설치 완료 후, konlpy 라이브러리가 잘 설치되었는지 확인해보았다.\nimport konlpy mecab = konlpy.tab.Mecab() print(mecab.pos(\u0026#34;아버지가 방에 들어가신다.\u0026#34;)) 확인 결과 다음과 같이 출력이 잘 되었다.\ngunicorn을 실행하기 위하여 wsgi.py 를 만든 뒤, 다음의 명령어를 입력한다.\n wsgi.py  gunicorn --bind 0.0.0.0:5000 wsgi:app 이제 시스템에 서비스를 등록하고, 관련 설정을 해준다.\nsudo vi /etc/systemd/system/flask_konlpy_webapp.service [Unit] Description=Gunicorn instance to serve flask_konlpy_webapp After=network.target [Service] User=opc Group=nginx WorkingDirectory=/home/opc/flask_konlpy_webapp Environment=\u0026#34;PATH=/home/opc/flask_konlpy_webapp/venv/bin\u0026#34; ExecStart=/home/opc/flask_konlpy_webapp/venv/bin/gunicorn --workers 3 -t 120 --bind unix:flask_konlpy_webapp.sock -m 007 wsgi:app [Install] WantedBy=multi-user.target 서비스를 시작하고 서비스의 상태를 확인해보자.\nsudo systemctl start flask_konlpy_webapp.service sudo systemctl enable flask_konlpy_webapp.service sudo systemctl status flask_konlpy_webapp.service Active: active (running)으로 잘 실행되고 있음을 확인할 수 있다.\nNginx 관련 설정은 다음과 같이 진행한다.\nsudo vi /etc/nginx/nginx.conf Server 부분을 찾아서 다음과 같이 추가 및 수정해준다.\nserver_name 부분의 server_domain_or_IP는 본인의 환경에 맞게 설정해준다.\nserver { listen 80; server_name server_domain_or_IP; include /etc/nginx/default.d/*.conf; location / { proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_http_version 1.1; proxy_set_header Connection \u0026#34;\u0026#34;; proxy_pass http://unix:/home/opc/flask_konlpy_webapp/flask_konlpy_webapp.sock; } error_page 404 /404.html; location = /40x.html { } error_page 500 502 503 504 /50x.html; location = /50x.html { } } Nginx 관련 접근 권한 및 설정을 변경해준다.\nsudo usermod -a -G opc nginx chmod 710 /home/opc sudo nginx -t Nginx 설정이 완료되었다면, Nginx 서비스를 시작해주도록 하자.\nsudo systemctl start nginx sudo systemctl enable nginx Active: active (running)으로 잘 실행되고 있음을 확인할 수 있다.\n서비스 서비스는 링크를 선택하면 사용할 수 있고, 링크를 선택하면 다음과 같은 화면이 나온다.\n아무 소개가 없으면 너무 허전할 것 같아 간단한 소개와 형태소 분석 입출력 창으로 이동하는 버튼을 만들어 놓았다. (그래도 휑해보이는 건 디자인 감각이 부족한 탓으로 돌려본다\u0026hellip;ㅠㅠ)\nClick Me ! 라는 (다소 어이없는) 문구의 버튼을 클릭하면 형태소 분석 입출력 창으로 이동하게 되며, 원하는 형태소 분석기를 선택 후 Analysis 버튼을 누르면 해당되는 형태소 분석기를 통해 분석된 결과가 분석 결과창에 출력되도록 했다.\n형태소 분석기의 경우 다음의 5가지 중 하나를 선택할 수 있고, 각 형태소 분석기마다의 결과는 다음과 같다. 입력 문장은 \u0026lsquo;아버지가방에들어가셨다.\u0026lsquo;로 하였으며, 띄어쓰기를 하지 않은 이유는 형태소 분석기들이 \u0026lsquo;아버지가/방에\u0026rsquo;로 분석할지 \u0026lsquo;아버지/가방에\u0026rsquo;로 분석할지 궁금해서였다. 분석 결과는 어절 단위와 음절 단위 중 이용하는 형태소 분석기에 따라 입력 값의 전처리 방향에 대해 조금이나마 도움을 줄 수 있지 않을까 싶었다.\n  꼬꼬마\n  \u0026lsquo;아버지\u0026rsquo;와 \u0026lsquo;가방\u0026rsquo;으로 분석되었다.\n      코모란\n  \u0026lsquo;아버지\u0026rsquo;와 \u0026lsquo;가방\u0026rsquo;으로 분석되었다.\n      한나눔\n  \u0026lsquo;아버지가방에들어가\u0026rsquo;로 분석된 것으로 보아 음절 단위의 분석은 다소 어려움이 있는 것으로 보인다.\n      Mecab\n  \u0026lsquo;아버지\u0026rsquo; 와 \u0026lsquo;방\u0026rsquo;으로 분석되었다.\n      Okt\n  \u0026lsquo;아버지\u0026rsquo; 와 \u0026lsquo;가방\u0026rsquo;으로 분석되었다.\n      결과 물론, 예시 문장 하나만으로 어떠한 형태소 분석기가 음절 단위의 형태소 분석을 모두 잘 해낸다와 같은 결과는 과도한 해석일 것이다. 다만, konlpy에서 지원하는 한국어 형태소 분석기는 어절 단위의 분석이 음절 단위의 분석보다 분석 정확도가 높으므로 형태소 분석기를 사용할 때 입력 값을 적절히 전처리 해주면 보다 좋은 분석 결과를 도출할 수 있다라는 (당연한 결과는) 것은 알 수 있었다.\n물론 해당 서비스의 개발 목적이 형태소 분석기 자체가 아니라 Flask + gunicorn + Nginx 조합으로 웹 애플리케이션 서비스를 배포해보자 라는 목적이었기 때문에 일련의 성과는 얻을 수 있었다고 생각한다. 간단한 형태의 서비스도 이렇게나 머리 아픈데 완전한 형태의 서비스를 만들고, 지속적으로 유지보수 및 업데이트 해나가는 개발자 분들에게 존경을 표하는 바이다\u0026hellip;\n후기  KKma와 okt가 다른 형태소 분석기에 비해 유독 느리다. 일정 시간이 넘어버리면 timeout이 되어 아예 오류 페이지로 이동해버린다. (ㅠㅠ) .sock 서비스 접근 권한 오류가 발생했었다. 처음엔 에러 원인도 몰랐다가 error_log를 찍어서 보니 접근 권한 오류가 발생한 것을 확인할 수 있었고, 수정했다. Oracle Cloud의 무료 티어가 생각보다 쓸만하다는 생각이 계속 들었다. 간단한 서비스라면 무료 티어를 활용하는 방안도 하나의 선택지가 될 것으로 보인다. 설계 자체에 대한 아쉬움이 계속 남는다. 석사과정 당시엔 관련 서비스 설계에 대한 지식이 부족했고, 돌아만 가면 되지라는 생각으로 만들었던 서비스이기 때문에 완성도가 굉장히 조악하다. 만약 지금이라면 구조 자체를 형태소 분석기 모델이 돌아가는 RESTAPI 서버 하나와 React or Vue로 구성된 FrontEnd 서버 하나로 이원화시켜 운영할 것 같다. Backend는 값에 대해서만 처리하고, Frontend에서는 입출력단만 처리하여 뿌릴 수 있도록 하는 형태로 하면 어땠을까 생각해본다. 개발된 코드는 Github에 올려두었다. 굉장히 조악하고, 미흡한 코드지만 조금이나마 도움이 되었으면 한다\u0026hellip;  Reference  https://www.digitalocean.com/community/tutorials/how-to-serve-flask-applications-with-gunicorn-and-nginx-on-centos-7  ","permalink":"https://zellyshu.github.io/posts/2021-06-01-flakonlpy/","summary":"Intro 본 포스팅은 Python의 한국어 형태소 분석 라이브러리인 konlpy를 활용하여 CentOS 7 + Flask + gunicorn + Nginx 조합으로 웹 애플리케이션 서비스를 만들어본 토이 프로젝트 개발기다. 개발된 서비스는 링크(클릭)를 통하여 이동하면 사용할 수 있으나, 이용자에게 제공되는 완전하고 안정된 형태의 서비스라기보다 Oracle Cloud의 무료 티어를 활용해보기 위한 테스트 목적이 강하므로 이 점을 인지하여 준다면 감사할 것 같다. (갑자기 다운되거나 오류 발생 확률 매우매우매우 높음ㅠㅠ)\n왜 만들었나? 취직하여 일을 시작한 지 얼마 되지 않았을 무렵, 사내 팀에서 AWS를 사용하고 있어 자연스레 클라우드 환경에 관심이 많아진 상태였다.","title":"CentOS 7 + Flask + gunicorn + Nginx 조합으로 Oracle Cloud에서 돌아가는 서비스 만들어보기"},{"content":"Intro 본 포스팅은 이전 Oracle Cloud 가입과 간단한 설정에 관하여 작성했던 포스트의 IBM 버전이다. IBM Cloud의 가입과 Free Tier인 Lite 계정에서 클라우드 초심자가 개발 환경을 설정해 본 경험을 (= 미래의 내가 잊어먹지 않도록 정리해둔 것을) 다룬 것이다. 나와 같은 분들과 IBM Cloud를 처음 사용하시는 분들께 조금이나마 참고 자료로서 도움이 되었으면 하는 마음에 글을 작성하였다.\nIBM Cloud란? IBM Cloud는 어디선가 한 번쯤은 들어봤을 IBM사에서 운영하는 Cloud Flatform으로, 현재 무료 티어인 Lite 계정을 지원하고 있다. 특히 무료 티어에서도 2GB의 데이터 스토리지, 1 vCPU 제공 등 무료임에도 준수한 서비스들을 제공하고 있다는 것이 특징이다. 또한, Free Tier인 Lite 계정의 경우 기간 제한이 없고, 신용 카드 정보도 필요하지 않다는 점을 내세우고 있다.\n가입 가입은 과정을 따라 진행하면 되며, 메일로 code를 인증하면 계정 생성이 완료된다.\n로그인하게 되면 여타 클라우드 서비스와 마찬가지로 대시보드 화면을 마주할 수 있게 된다.\n설정 및 샘플 앱 생성 대시보드 화면에서 좌측 상단의 메뉴를 클릭하면 위와 같이 숨겨져 있던 메뉴들이 주루룩 나오는데, Cloud Foundry메뉴에서 vCpu를 생성할 수 있으므로 클릭해주도록 하자.\n클릭하면 설정하는 화면으로 이동하게 되며, 해당 화면에서 생성하고자 하는 앱과 리소스 구성에 대한 설정을 손쉽게 할 수 있다. 별도의 Shell 환경에서의 지정을 거치지 않아도 된다는 게 아주 편했다.\n설정 화면을 넘어가면, 샘플 앱에 대한 이름과 위치(국가), 요금제 등을 선택하는 화면이 출력된다.\nLite 계정은 최대 256MB의 메모리까지 사용 가능하다.\n무료 티어를 체험하는 것이 목적이므로, 플랜에 맞는 요금제를 선택하면, 어떤 언어와 자원으로 앱을 구성할 것인지를 선택하게 된다. Node.js 테스트 목적으로 환경을 구성할 것이기 때문에 SDK for Node.js 를 선택하고, 간단한 태그를 기입해주었다.\n설정이 완료되면 다음과 같은 화면을 마주할 수 있다.\n그리고 앱 URL 방문을 클릭하여 URL로 이동하면, 반가운 Hello World 화면을 마주할 수 있다.\nNode.js Repository Clone IBM github에서 get-started-node 레포지토리를 클론해서 Node.js 샘플 프로젝트를 실행해 볼 수 있다. 클론해서 관련 의존성 자료들을 설치하고,\nnpm install npm start 를 해주면 3000번 포트에 서비스가 돌아가고 있다는 결과를 받을 수 있다. https://localhost:3000 에 접속하면 다음과 같은 화면이 나오는 것을 확인할 수 있다.\nIBM Cloud CLI 를 통하여 배포하기 IBM 홈페이지의 IBM Cloud CLI 시작하기 문서는 IBM Cloud CLI를 통하여 Foundry 환경에 배포하는 방법을 단계적으로 설명하고 있다. 우선, 운영체제에 맞는 IBM Cloud CLI 를 설치한다.\nMacOS or Linux curl -sL https://raw.githubusercontent.com/IBM-Cloud/ibm-cloud-developer-tools/master/linux-installer/idt-installer | bash Windows 10 Pro (PowerShell 관리자 모드로 실행) [Net.ServicePointManager]::SecurityProtocol = \u0026#34;Tls12, Tls11, Tls, Ssl3\u0026#34;; iex(New-Object Net.WebClient).DownloadString(\u0026#39;https://raw.githubusercontent.com/IBM-Cloud/ibm-cloud-developer-tools/master/windows-installer/idt-win-installer.ps1\u0026#39;) 설치가 완료되면 명령어를 통하여 로그인하여 지역설정을 해주자. 지역은 Foundry를 생성할 때 설정했던 지역과 동일하게 선택해주도록 하자. (ex. 댈러스 = us-south)\nibmcloud login 로그인이 완료되면, Cloud Foundry 앱 설정 관련 파일인 manifest.yml 을 변경해주어야 한다. 앞서 생성한 Cloud Foundry에 맞게 설정해주어야 하며 설정을 제대로 해주지 않으면 앱이 하나 더 생겨서 메모리 아웃 (에러 정리 3번, 메모리 부족 참고)에러가 발생하므로 제대로 설정해주도록 하자.\n설정을 완료하였다면 다음 명령어를 통하여 ibmcloud에 배포하면 된다.\nibmcloud target --cf ibmcloud cf push 로그에 찍히는 URL로 접근해보면 아까 레포지토리를 클론해서 배포 테스트했던 화면이 나오는 것을 확인할 수 있다. 만약 나오지 않는다면 리소스 그룹 설정 및 포트 관련 설정을 한 번 체크해보자..ㅠㅠ\n설치 중 발생했던 에러 정리 리소스 그룹 관련 에러 다음과 같이 \u0026lsquo;대상으로 지정된 리소스 그룹이 없습니다.\u0026rsquo; 라는 에러가 발생하였다.\n리소스 지정만 해주면 된다.\nibmcloud target -g Default  IBM Cloud Foundry를 통해 만들어둔 app 지정이 안되어 있었다는 것을 알았더라면 쉽게 해결했을 문제\u0026hellip;  \u0026lsquo;Cloud Foundry CLI가 IBM Cloud CLI에 설치되지 않았습니다.\u0026rsquo; ibmcloud cf install 메모리 부족 메모리가 부족하다면서 에러가 발생할 때 ibm cloud 대시보드에서 app이 2개가 올라가진 않았는지 체크해보자.\n manifest.yml 에서 app name을 안바꿔서 get-started-node가 별도의 app 으로 올라가서 메모리 아웃이 발생한 상황 로그인하고, manifest.yml 파일을 열고 app name을 안바꿔주면 새로운 Cloud Foundry 앱을 만들어버린다는 점 상기할 것.  ","permalink":"https://zellyshu.github.io/posts/2021-04-15-ibm-cloud/","summary":"Intro 본 포스팅은 이전 Oracle Cloud 가입과 간단한 설정에 관하여 작성했던 포스트의 IBM 버전이다. IBM Cloud의 가입과 Free Tier인 Lite 계정에서 클라우드 초심자가 개발 환경을 설정해 본 경험을 (= 미래의 내가 잊어먹지 않도록 정리해둔 것을) 다룬 것이다. 나와 같은 분들과 IBM Cloud를 처음 사용하시는 분들께 조금이나마 참고 자료로서 도움이 되었으면 하는 마음에 글을 작성하였다.\nIBM Cloud란? IBM Cloud는 어디선가 한 번쯤은 들어봤을 IBM사에서 운영하는 Cloud Flatform으로, 현재 무료 티어인 Lite 계정을 지원하고 있다.","title":"IBM Cloud 가입부터 간단한 설정까지"},{"content":"Intro 대학원 시절 기계 학습한 모델을 활용하여 데모 서비스를 만들어 시연해야 할 일이 종종 있었다.\n당시, 개발한 형태소 분석기 모델을 가지고 다른 연구실처럼 웹에 형태소 분석기를 띄워 데모 형태로 서비스를 하자는 의견이 나왔고, 서비스 구현 파트를 맡게 되었다. (이 때는, input으로 입력값을 받도록 하고 개발 툴에서 run 하면 이용자도 들어와서 사용할 수 있을 거라는 발칙한 생각을 할 정도로 구조도 뭣도 아무것도 모르는 상황이었다.)\nPython도 아등바등 해나갔었는데 하물며 웹 프레임워크 관련 지식은 더더욱 기초지식조차 없었던 터라 일단 Python만 가지고 서비스를 만들어야겠다 생각했다.\n검색을 통하여 웹 프레임워크라는 것을 알게 되었고, Django 책을 사서 따라해봤으나 도저히 무슨 말인지 이해가 안되는 총체적 난국 그 자체였다.\n그래서 최대한 쉽고 빠르게 만들 수 있다는 Flask라는 웹 프레임워크를 통하여 서비스를 만들어보기 시작했다.\n하지만, 서비스가 돌아가는 구조 자체가 머릿 속에 그려지지 않았던 터라 이게 왜 안 나오고, 이게 왜 나오는지 이해하는데만 시간이 엄청 소요되었던 기억이 난다.\n(처음엔 HTML과 CSS, jquery만 가지고 만들다 나중엔 화려하게 만들어보겠다는 일념하에 D3.js 및 javascript를 열심히 활용하여 만들었다는 이야기는 언젠가 작성하는 걸로\u0026hellip;)\n졸업 이후에도 간단한 토이 프로젝트 서비스를 만들 때 Flask를 사용하곤 했는데 언젠가부터 검색할 때마다 FastAPI라는 웹 프레임워크가 같이 출현하는 결과가 많아지길래 한 번 설치하고 테스트해보자 마음먹게 되었다.\nFastAPI FastAPI는 현대적이고, 빠르며(고성능), 파이썬 표준 타입 힌트에 기초한 Python3.6+의 API를 빌드하기 위한 웹 프레임워크라고 공식문서에서 소개하고 있다.\n주요 장점은 다음과 같다.\n1. 빠른 속도  공식 문서에서는 NodeJS 및 Go와 대등할 정도로 매우 높은 성능을 자랑한다고 이야기 하고 있다.  2. 쉽고, 짧으며, 직관적  실제 예제 코드를 작성해보면서 Flask와 비슷하다는 느낌을 받았을 정도로 매우 경량화되어 있다.  3. 비동기 (Starlette 프레임워크 기반)  WSGI가 아닌 ASGI를 지원  4. API 문서 자동화 및 작성을 위한 Swagger UI, Redoc 제공  개인적으로, 기존 Flask로 구현하였던 서비스를 FastAPI로 바꿔보아야겠다는 생각이 들게 한 큰 부분이었다.  추가적인 사항으로는 Python 3.6 이상의 버전을 지원하고 있고, 라이센스는 MIT License를 따르고 있다.\n설치 테스트는 WSL2에 설치한 Ubuntu 20.04 LTS 환경에서 진행하였고, Conda 가상환경(Python 3.7.6)에서 설치하였다.\n설치 및 테스트  설치 및 테스트는 공식 문서를 참고했다.  1. conda 가상환경 생성  python 3.7 버전에서 테스트  conda create -n fastapitest python=3.7 2. conda 가상환경 활성화 conda activate fastapitest 3. pip로 FastAPI 및 Uvicorn 설치 pip install fastapi pip install uvicorn[standard] 4. main.py 생성 from typing import Optional from fastapi import FastAPI app = FastAPI() @app.get(\u0026#34;/\u0026#34;) def read_root(): return {\u0026#34;Hello\u0026#34;: \u0026#34;World\u0026#34;} @app.get(\u0026#34;/items/{item_id}\u0026#34;) def read_item(item_id: int, q: Optional[str] = None): return {\u0026#34;item_id\u0026#34;: item_id, \u0026#34;q\u0026#34;: q} 5. 서버 실행 uvicorn main:app --reload 6. GET 확인 http://127.0.0.1:8000/items/5?q=somequery 7. main.py 내용 추가 from typing import Optional from fastapi import FastAPI // 추가 from pydantic import BaseModel app = FastAPI() // 추가 class Item(BaseModel): name: str price: float is_offer: Optional[bool] = None @app.get(\u0026#34;/\u0026#34;) def read_root(): return {\u0026#34;Hello\u0026#34;: \u0026#34;World\u0026#34;} @app.get(\u0026#34;/items/{item_id}\u0026#34;) def read_item(item_id: int, q: Optional[str] = None): return {\u0026#34;item_id\u0026#34;: item_id, \u0026#34;q\u0026#34;: q} // 추가 @app.put(\u0026#34;/items/{item_id}\u0026#34;) def update_item(item_id: int, item: Item): return {\u0026#34;item_name\u0026#34;: item.name, \u0026#34;item_id\u0026#34;: item_id} 8. 대화형 API 문서 확인  —reload 옵션으로 인하여, 자동으로 업데이트 됨  http://127.0.0.1:8000/docs   PUT 메소드 내 매개변수들을 업데이트 함으로써 내용이 수정되는 것을 확인할 수 있음 (Try it out - Execute)\n  9. Alternative Docs (대안 문서) 확인  —reload 옵션으로 인하여, 자동으로 업데이트 됨  http://127.0.0.1:8000/redoc etc  공식 문서가 상당히 잘 기술되어 있는 것이 특징 FastAPI 의 소개와 설치 및 테스트, 헤더 매개변수 등의 경우 한글화가 되어 있으나 다른 부분은 한글화가 이루어지지 않은 상태. (2021년 04월 08일 기준)  Reference  https://fastapi.tiangolo.com/ko/ https://github.com/tiangolo/fastapi  ","permalink":"https://zellyshu.github.io/posts/2021-04-08-fastapi-install-test/","summary":"Intro 대학원 시절 기계 학습한 모델을 활용하여 데모 서비스를 만들어 시연해야 할 일이 종종 있었다.\n당시, 개발한 형태소 분석기 모델을 가지고 다른 연구실처럼 웹에 형태소 분석기를 띄워 데모 형태로 서비스를 하자는 의견이 나왔고, 서비스 구현 파트를 맡게 되었다. (이 때는, input으로 입력값을 받도록 하고 개발 툴에서 run 하면 이용자도 들어와서 사용할 수 있을 거라는 발칙한 생각을 할 정도로 구조도 뭣도 아무것도 모르는 상황이었다.)\nPython도 아등바등 해나갔었는데 하물며 웹 프레임워크 관련 지식은 더더욱 기초지식조차 없었던 터라 일단 Python만 가지고 서비스를 만들어야겠다 생각했다.","title":"FastAPI 설치 및 테스트"},{"content":"Intro MySQL, Oracle, MSSQL 등의 DB는 접해보았으나 PostgreSQL은 이야기만 들어보고 실제 사용해 본 경험은 없어서 설치하고, 테스트 목적의 테이블을 생성하고, DB를 삽입하여 조회해보는 작업을 해보았다.\nPostgreSQL은 오픈 소스 객체-관계형 데이터베이스 관리 시스템(ORDBMS) 이다. 평소 관심만 가지고 있다가 추후 토이 프로젝트 시 DB를 선택할 때 PostgreSQL을 도입해보고 싶다는 막연한 생각에 맨땅에 헤딩하듯이 설치 테스르를 진행한 것이다.\nDB에 조예가 깊지 않아 설치하고, 잘 돌아가는지 정도의 레벨로 테스트를 진행하였고 관련 내용을 간략하게 정리한 것임을 미리 알린다.\nWSL2에 설치한 Ubuntu 20.04 LTS 환경에서 테스트를 진행하였고, Linux 환경에 PostgreSQL 12.6을 설치하였다.\n설치  Ubuntu 20.04 (WSL2 에서 진행함)  1. apt-get update 진행 sudo apt-get update 2. apt-get install 명령어를 통하여 postgresql, postgresql-contrib을 설치 sudo apt-get install postgresql postgresql-contrib 3. postgres 계정 로그인 sudo -i -u postgres 4. psql 명령어로 DB 접근  CREATE, ALTER 등의 명령어 실행 테스트  psql exit 5. psqladmin 계정과 psql_db 생성 psql -h localhost -U psqladmin -d psql_db 6. 스키마, 시퀀스, 테이블 생성  https://racoonlotty.tistory.com/entry/Ubuntu%EC%97%90-PostgreSQL-%EC%84%A4%EC%B9%98-1?category=868396 포스팅 참고 (6~12)  7. 시퀀스 변경 ALTERSEQUENCEmenu_id_seqOWNEDBYmenu.id;8. 인덱스 생성 CREATEUNIQUEINDEX\u0026#34;UNIQUE\u0026#34;ONusers(email);9. 데이터 삽입 1 10. 데이터 삽입 2 11. \u0026lsquo;menu\u0026rsquo; 테이블 select select*frommenu;​\n​\n​\n​\n12. 릴레이션 리스트 확인 ​\n​\n\\d​\n설치 시 발생했던 에러정리 1. socket 관련 에러 (psql 명령어로 DB접근 시 socket 에러 발생) 1. https://stackoverflow.com/questions/31645550/postgresql-why-psql-cant-connect-to-server의 3번째 답변으로 해결 (postgres service를 재시작) ​ ​\nsudo /etc/init.d/postgresql restart ​ ​ ​ ​ 추가로 익혀야 할 사항들  기본키가 자동으로 증가되도록 하려면?  nextval (regclass), serial 등 사용 참고자료 URL  https://mine-it-record.tistory.com/341      Reference  https://racoonlotty.tistory.com/entry/Ubuntu%EC%97%90-PostgreSQL-%EC%84%A4%EC%B9%98-1?category=868396 https://dejavuqa.tistory.com/16 https://postgresql.kr/  ","permalink":"https://zellyshu.github.io/posts/2021-04-02-postgresql-install-test/","summary":"Intro MySQL, Oracle, MSSQL 등의 DB는 접해보았으나 PostgreSQL은 이야기만 들어보고 실제 사용해 본 경험은 없어서 설치하고, 테스트 목적의 테이블을 생성하고, DB를 삽입하여 조회해보는 작업을 해보았다.\nPostgreSQL은 오픈 소스 객체-관계형 데이터베이스 관리 시스템(ORDBMS) 이다. 평소 관심만 가지고 있다가 추후 토이 프로젝트 시 DB를 선택할 때 PostgreSQL을 도입해보고 싶다는 막연한 생각에 맨땅에 헤딩하듯이 설치 테스르를 진행한 것이다.\nDB에 조예가 깊지 않아 설치하고, 잘 돌아가는지 정도의 레벨로 테스트를 진행하였고 관련 내용을 간략하게 정리한 것임을 미리 알린다.","title":"PostgreSQL 설치 테스트 (Ubuntu 20.04, WSL2)"},{"content":"Intro 본 포스팅은 Oracle Cloud 의 가입과 Free Tier에서 클라우드 초보자의 입장에서 개발 환경을 설정해 본 경험을 (= 미래의 내가 잊어먹지 않도록 정리해둔 것을) 다룬 것이다. 나와 같은 분들과 Oracle Cloud를 처음 사용하시는 분들께 조금이나마 참고 자료로서 도움이 되었으면 하는 마음에 글을 작성하였다.\nOracle Cloud란? Oracle Cloud는 어디선가 한 번쯤은 들어봤을 Oracle사에서 운영하는 Cloud Flatform으로, GCP, AWS와 비교하여 Free Tier를 2개나 지원하고 있다. 특히 Free Tier에서도 DB 2개, 컴퓨팅 VMs, 100GB 블록의 볼륨, 10GB 객체 스토리지를 제공하고 있다는 것이 특장점이다. 이는 이미 시장을 점유하고 있는 AWS, GCP, Azure와 비교해도 전혀 사용에 있어 지장 없는 스펙이라 할 수 있다.\n가입 가입은 과정을 따라 진행하면 되는데 Cloud Tenant 라고 이름을 설정하는 부분이 있다. 요것도 기억을 해주어야 나중에 로그인할 때 헤매지 않는다\u0026hellip;\n설정 로그인하게 되면 다른 클라우드 서비스에서 으레 볼 수 있는 대시보드 화면을 마주하게 된다.\n위의 사진을 보면 서울 지역으로 생성한 것을 확인할 수 있고, (서울 TO가 부족하여 생성이 안되는 경우가 종종 있다고 하니 이 점 참고) 본래 목적인 VM 인스턴스를 생성하여 SSH로 어디서든 접근하여 사용할 수 있도록 설정하였다. (토이 프로젝트용 1개, 개인 웹서버용 1개를 생성하여 현재 무리 없이 사용 중에 있다.)\n물론, 웹에서도 터미널을 띄워 개발할 수 있지만 상당히 불편하기 때문에 개발을 용이하게 하기 위하여 SSH 접속 환경을 만들어주었다. (Window 환경에서 진행)\nSSH 접속을 위하여 Puttygen으로 SSH key를 생성하고, 이를 생성한 인스턴스에 설정한 뒤 Putty를 이용하여 접근하니 하단 이미지와 같이 접속이 잘 되는 것을 확인할 수 있었다. 참고로, username의 default값은 centos의 경우 opc이며, ubuntu의 경우에는 ubuntu로 설정되어 있다.\n그리고, 다른 클라우드 플랫폼과 마찬가지로 방화벽 포트를 추가적으로 열어주어야 특정 서비스의 접근 및 외부 환경에서의 접근이 가능하다. 방화벽 포트는 \u0026lsquo;수신 규칙\u0026rsquo; 및 \u0026lsquo;송신 규칙\u0026rsquo;에서 변경할 수 있으며, 필요한 포트만 열어 보안 문제를 신경 쓰는 것이 좋을 것으로 보인다.\n","permalink":"https://zellyshu.github.io/posts/2021-03-31-oracle-cloud/","summary":"Intro 본 포스팅은 Oracle Cloud 의 가입과 Free Tier에서 클라우드 초보자의 입장에서 개발 환경을 설정해 본 경험을 (= 미래의 내가 잊어먹지 않도록 정리해둔 것을) 다룬 것이다. 나와 같은 분들과 Oracle Cloud를 처음 사용하시는 분들께 조금이나마 참고 자료로서 도움이 되었으면 하는 마음에 글을 작성하였다.\nOracle Cloud란? Oracle Cloud는 어디선가 한 번쯤은 들어봤을 Oracle사에서 운영하는 Cloud Flatform으로, GCP, AWS와 비교하여 Free Tier를 2개나 지원하고 있다. 특히 Free Tier에서도 DB 2개, 컴퓨팅 VMs, 100GB 블록의 볼륨, 10GB 객체 스토리지를 제공하고 있다는 것이 특장점이다.","title":"Oracle Cloud 가입부터 간단한 설정까지"},{"content":"직장인에게 있어 메일관리는 필수적이라 할 수 있다. 대학생 때까지야 메일 계정 하나 가지고 개인 용도로도 사용하고, 조별 과제도 하고, 대외활동도 하고 하였으나 대학원을 다니며 자연스레 개인용 계정과 업무용 계정이 분리되게 되었다.\n대학원 졸업 이후 회사에 들어가니 회사 메일 계정을 받게 되고, 파견을 나가니 파견회사의 메일 계정을 또 만들게 되고\u0026hellip; 그렇게 프로젝트를 몇 개 진행하다보니 어느새 메일 계정 여러 개를 동시에 확인해야 하는 상황에 이르게 되었다.\n이렇게 업무용 뿐만 아니라 개인용도까지 메일을 여러 개 사용해야 하는 경우가 생기게 되면서 하나의 플랫폼에서 확인하면 편하겠다라는 생각이 들었다. 하나의 플랫폼에서 메일을 확인하고 메일을 보낼 수만 있으면 좋겠다는 바램이 컸다. 그러한 툴을 알아보던 중 단연 범용적인 툴은 Microsoft 사에서 나온 Outlook이었다.\n하지만, 윈도우와 맥, 더 나아가 리눅스도 종종 사용하는데 Microsoft Office를 모든 기기에 설치해서 사용하지는 않기 때문에 Outlook은 적합하지 않았다. (사실 괜한 마음에 오픈소스인 대체재를 사용하고 싶은 마음이 더 컸던 건지도 모른다\u0026hellip;) OS환경에 최대한 구애받지 않으면서도 오픈소스인 툴을 찾다가 눈에 띈 것이 바로 모질라 재단에서 개발한 오픈소스 이메일 클라이언트 툴인 모질라 썬더버드. 애칭 \u0026lsquo;천둥새\u0026rsquo;였다.\n한글화가 잘 되어있어 설치에는 어려움이 없는 편이나 관련 한글 자료가 많은 편은 아니라 직접 설치하면서 깨우친 점들이 몇 가지 있었다. 또한, 기존에 메일 관리 클라이언트를 아이폰 환경에서 Spark를 사용해본 것이 전부이기 때문에 PC 환경에서 메일 관리 클라이언트를 처음 사용해본다고 봐도 무방한 뉴비임에도, 메일 계정을 등록하고, 서명 및 관련 설정들을 하는 데 큰 어려움이 없던 걸 보면 사용에 큰 진입장벽이 있다고는 볼 수 없을 것 같다. Outlook을 아예 사용해보지 않아서 직접적인 비교는 어려우나 이전에 맥 환경에서 사용했던 Spark와 비교해볼 때 편리함이라는 면과 메일을 받아오는 속도 같은 측면에서 불편함은 느껴지지 않았다.\nChrome Extension처럼 확장 기능과 테마들도 지원하고 있으며 메일을 필터링해서 받아오는 등의 작업도 가능하다. 또한, 전체적인 화면이 웹 브라우저처럼 Tab화면으로 구분되고, UI도 웹 브라우저와 흡사하여 낯설다는 느낌을 받지 않았던 것도 특징이라 할 수 있겠다.\n현재 회사 업무용 그룹웨어 메일계정과 업무+개인 gmail 계정들을 등록해두고 활용하고 있으며, 메일마다 서명을 별도 저장할 수도 있어 썬더버드 내에서 모든 메일 발송 작업이 가능하도록 해두었다. (다만, 썬더버드에서 메일을 보내면 SMTP를 활용하고 있는 회사 계정은 보낸 편지함에 등록이 되지 않는 점과 수신확인이 불가능하다는 점 때문에 메일 알림이 오면 확인 뒤 메일을 작성하고, 메일 발송 자체는 회사 그룹웨어로 하고 있다ㅠㅠ)\n만약 Outlook의 대체재를 찾는다거나, 혹은 Microsoft Office를 사용하기 어려운 상황이라면 썬더버드를 사용해보는 것은 어떨까 추천해본다. 추가적으로, 모질라 재단에서 발표한 2020년 재무 보고서에 의하면 썬더버드를 개발함에 있어 꽤나 건강하게 운영 중이라고 한다는 TMI까지 덧붙여본다. (GeekNews를 통해 알게 된 내용! 항상 잘 읽고 있습니다. 감사합니다.)\n","permalink":"https://zellyshu.github.io/posts/2021-03-30-mozilla-thunderbird/","summary":"직장인에게 있어 메일관리는 필수적이라 할 수 있다. 대학생 때까지야 메일 계정 하나 가지고 개인 용도로도 사용하고, 조별 과제도 하고, 대외활동도 하고 하였으나 대학원을 다니며 자연스레 개인용 계정과 업무용 계정이 분리되게 되었다.\n대학원 졸업 이후 회사에 들어가니 회사 메일 계정을 받게 되고, 파견을 나가니 파견회사의 메일 계정을 또 만들게 되고\u0026hellip; 그렇게 프로젝트를 몇 개 진행하다보니 어느새 메일 계정 여러 개를 동시에 확인해야 하는 상황에 이르게 되었다.\n이렇게 업무용 뿐만 아니라 개인용도까지 메일을 여러 개 사용해야 하는 경우가 생기게 되면서 하나의 플랫폼에서 확인하면 편하겠다라는 생각이 들었다.","title":"모질라 썬더버드 사용기"}]